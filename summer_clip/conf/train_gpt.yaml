defaults:
  - hydra_setup
  - meta_setup
  - saved_paths: train_gpt
  - _self_

clip:
  tokenizer_id: openai/clip-vit-base-patch32  # should not differ from large

clip_gpt:
  class_path: summer_clip.clip_prompt.gpt.ClipGPTFull
  gpt_model_id: gpt2-large
  clip_model_name: RN50
  adapters:
    emb_hid_dim: 256
    head_hid_dim: null  # tied adapter with emb layer

dataset:
  train:
    dataset:
      dataset_path: ${saved_paths.tokenized_datasets.openwebtext}
    subpart: null  # 0.1  # if null, all is taken
  val:
    dataset:
      path: wikitext
      name: wikitext-2-raw-v1
      split: test
    max_length: 80
    text_column: text
    filter:
      _target_: summer_clip.clip_prompt.train_gpt.WikiFilter
      text_column: ${dataset.val.text_column}

data_loader:
  train:
    batch_size: 36
    shuffle: true
    pin_memory: true
    num_workers: 8
  val:
    batch_size: 36
    shuffle: false
    pin_memory: false
    num_workers: 8

optim:
  weight_decay: 0.1
  adamw_kwargs:
    lr: 5e-4

scheduler:
  name: cosine
  warmup_part: 0.05

pretrained:
  model: null
  optimizer: null
  scheduler: null

accelerator:
  gradient_accumulation_steps: 16

training:
  epochs_num: 4
  info_steps: 1
  evals_per_epoch: 10
  clip_grad_norm: 1.0
  checkpoints_dir: checkpoints/

log:
  calculate_every: 1

exp:
  project: train_clip_gpt
  name: clip_gpt_v1
