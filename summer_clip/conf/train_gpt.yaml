defaults:
  - hydra_setup
  - meta_setup
  - saved_paths: train_gpt
  - _self_

clip:
  tokenizer_id: openai/clip-vit-base-patch32  # should not differ from large

clip_gpt:
  class_path: summer_clip.clip_prompt.gpt.ClipGPT
  gpt_model_id: gpt2-large
  clip_model_name: RN50
  adapters:
    emb_hid_dim: 256
    head_hid_dim: null  # tied adapter with emb layer

dataset:
  train:
    dataset:
      dataset_path: ${saved_paths.tokenized_datasets.openwebtext}
    subpart: 0.001  # if null, all is taken
  val:
    dataset:
      path: wikitext
      name: wikitext-2-raw-v1
      split: test
    max_length: 80
    text_column: text
    filter:
      _target_: summer_clip.clip_prompt.train_gpt.WikiFilter
      text_column: ${dataset.val.text_column}

data_loader:
  train:
    batch_size: 56
    shuffle: true
    pin_memory: true
    num_workers: 8
  val:
    batch_size: 56
    shuffle: false
    pin_memory: true
    num_workers: 8

optim:
  weight_decay: 0.1
  adamw_kwargs:
    lr: 5e-4

scheduler:
  name: cosine
  warmup_part: 0.05

accelerator:
  gradient_accumulation_steps: 4

training:
  epochs_num: 15
  info_steps: 1
  evals_per_epoch: 5
  clip_grad_norm: 1.0
  checkpoints_dir: checkpoints/

log:
  calculate_every: 1

exp:
  project: train_clip_gpt
  name: clip_gpt_v1
