defaults:
  - hydra_setup
  - meta_setup
  - _self_
  - dataset: imagenet_train_no_image
  - prompting: tip_imagenet

clip:
  model_name: RN50

tokenizer:
  # path: transformers.CLIPTokenizer
  path: transformers.GPT2Tokenizer
  # name: openai/clip-vit-base-patch32  # should not differ from large
  name: gpt2-large
  set_pad_as_eos: true
  tokenize_classes_kwargs:
    add_prefix_space: true

model:
  use_clip_gpt: false
  meta_cfg_path: /home/myurachinskiy/CLIP/summer-clip/outputs/2023-02-02/22-43-01/checkpoints/epoch_4/step_222605/model_cfg.yaml
  state_dict_path: /home/myurachinskiy/CLIP/summer-clip/outputs/2023-02-02/22-43-01/checkpoints/epoch_4/step_222605/model.ckpt

# prompt_model:
#   _target_: summer_clip.clip_prompt.prompt_learner.FluentPromptModel
#   model_cfg:
#     cdist_kwargs: {}

prompt_model:
  _target_: summer_clip.clip_prompt.autoprompt_learner.AutoPromptModel
  model_cfg:
    num_cands: 64
    search_steps: 1

# init_prompter:
#   _target_: summer_clip.clip_prompt.prompt_learner.InitNumTokensPrompter
#   token: <|startoftext|>
#   length: 5

init_prompter:
  _target_: summer_clip.clip_prompt.prompt_learner.InitRandomPrompter
  length: 5

text_batcher:
  path: summer_clip.clip_prompt.prompt_learner.OneStrTextBatcher
  kwargs:
    class_str: lion

data_loader:
  train:
    batch_size: 20480
    shuffle: true
    pin_memory: true
    num_workers: 4

# optim:
#   weight_decay: 0.01
#   optimizer:
#     path: summer_clip.clip_prompt.prompt_learner.make_langevin_optim
#     kwargs:
#       optim_cfg:
#         base_optim:
#           path: torch.optim.AdamW
#           kwargs:
#             lr: 1.0
#         beta_start: 1e-8
#         beta_end: 1e-8

optim:
  weight_decay: 0.01
  optimizer:
    path: torch.optim.AdamW
    kwargs:
      lr: 1.0

scheduler:
  get_scheduler_fun: transformers.get_scheduler
  # get_scheduler_fun: summer_clip.clip_prompt.prompt_learner.LangevinScheduler
  name: constant
  warmup_part: 0.

collator:
  _target_: summer_clip.clip_prompt.prompt_learner.LeftPromptCollator

accelerator:
  gradient_accumulation_steps: 1

training:
  epochs_num: 2
  info_steps: 1
  max_top_prompts: 50
  new_top_prompts_each_epoch: false
  checkpoints_dir: checkpoints/

log:
  calculate_every: 1

exp:
  project: train_prompt
  name: prompt_v1
